package llm

import (
	"context"
	"fmt"

	"github.com/Azure/azure-sdk-for-go/sdk/azidentity"
	"github.com/invopop/jsonschema"
	openai "github.com/openai/openai-go"
	"github.com/openai/openai-go/azure"
	"github.com/openai/openai-go/option"

	log "github.com/sirupsen/logrus"
)


type OpenAIProvider struct {
	client       openai.Client
	systemPrompt string
	model        Model
}

type OpenAIProviderConfig struct {
	APIKey       string
	SystemPrompt string
	Model        Model
	AzureConfig *AzureConfig // Optional, for Azure OpenAI
}


func NewOpenAIProvider(conf OpenAIProviderConfig) *OpenAIProvider {
	if conf.APIKey == "" && conf.AzureConfig == nil {
		log.Fatal("API key or Azure configuration is required for OpenAI provider")
	}

	if conf.AzureConfig != nil {
		// Create Azure OpenAI provider
		op, err := NewAzureOpenAIProvider(*conf.AzureConfig, conf.SystemPrompt, conf.Model)
		if err != nil {
			log.Fatalf("Failed to create Azure OpenAI provider: %v", err)
		}
		return op
	}
	// Create Native OpenAI provider
	if conf.APIKey == "" {
		log.Fatal("API key is required for OpenAI provider")
	}

	return NewNativeOpenAIProvider(conf.APIKey, conf.SystemPrompt, conf.Model)
}
	

func NewNativeOpenAIProvider(apiKey string, systemPrompt string, model Model) *OpenAIProvider {
	// Pass apiKey as env var OPENAI_API_KEY, or use WithAPIKey if needed
	client := openai.NewClient(
		option.WithAPIKey(apiKey),
	)
	return &OpenAIProvider{
		client:       client,
		systemPrompt: systemPrompt,
		model:        model,
	}
}

type AzureConfig struct {
    Endpoint       string
    APIVersion     string
    APIKey         string // optional, fallback to token
    DeploymentName string // azure deployment name
}

func NewAzureOpenAIProvider(cfg AzureConfig, systemPrompt string, model Model) (*OpenAIProvider, error) {
    opts := []option.RequestOption{
        azure.WithEndpoint(cfg.Endpoint, cfg.APIVersion),
    }
    if cfg.APIKey != "" {
        opts = append(opts, azure.WithAPIKey(cfg.APIKey))
    } else {
        cred, err := azidentity.NewDefaultAzureCredential(nil)
        if err != nil {
            return nil, fmt.Errorf("failed to get Azure credential: %w", err)
        }
        opts = append(opts, azure.WithTokenCredential(cred))
    }
    client := openai.NewClient(opts...)
    return &OpenAIProvider{client: client, systemPrompt: systemPrompt, model: Model{Name: cfg.DeploymentName}}, nil
}

func GenerateSchema[T any]() interface{} {
	reflector := jsonschema.Reflector{
		AllowAdditionalProperties: false,
		DoNotReference:            true,
	}
	var v T
	return reflector.Reflect(v)
}

func (p *OpenAIProvider) RequestCompletion(prompt string) (string, error) {
	log.Debugf("Requesting completion from OpenAI with prompt: %s", prompt)
	ctx := context.Background()
	messages := []openai.ChatCompletionMessageParamUnion{}

	if p.systemPrompt != "" {
		messages = append(messages, openai.SystemMessage(p.systemPrompt))
	}
	messages = append(messages, openai.UserMessage(prompt))

	params := openai.ChatCompletionNewParams{
		Messages: messages,
		Model:    p.model.Name,
	}

	resp, err := p.client.Chat.Completions.New(ctx, params)
	if err != nil {
		return "", err
	}
	return resp.Choices[0].Message.Content, nil
}

func (p *OpenAIProvider) RequestCompletionWithJSONSchema(prompt string, schema any) (string, error) {
	ctx := context.Background()
	messages := []openai.ChatCompletionMessageParamUnion{}

	if p.systemPrompt != "" {
		messages = append(messages, openai.SystemMessage(p.systemPrompt))
	}
	messages = append(messages, openai.UserMessage(prompt))

	schemaParam := openai.ResponseFormatJSONSchemaJSONSchemaParam{
		Name:   "structured_output",
		Schema: schema, // expects a struct generated by github.com/invopop/jsonschema
		Strict: openai.Bool(true),
	}

	params := openai.ChatCompletionNewParams{
		Messages: messages,
		Model:    p.model.Name,
		ResponseFormat: openai.ChatCompletionNewParamsResponseFormatUnion{
			OfJSONSchema: &openai.ResponseFormatJSONSchemaParam{
				JSONSchema: schemaParam,
			},
		},
	}

	resp, err := p.client.Chat.Completions.New(ctx, params)
	if err != nil {
		return "", err
	}
	log.Debugf("OpenAI response: %s", resp.Choices[0].Message.Content)

	// Return raw JSON, user can unmarshal as needed
	return resp.Choices[0].Message.Content, nil
}

// func (p *OpenAIProvider) RequestCompletionWithJSONSchema(prompt string, jsonSchema *jsonschema.Schema) (string, error) {

// 	resp, err := p.client.CreateChatCompletion(
// 		context.Background(),
// 		openai.ChatCompletionRequest{
// 			Model: p.model.Name,
// 			Messages: []openai.ChatCompletionMessage{
// 				{
// 					Role:    openai.ChatMessageRoleSystem,
// 					Content: p.systemPrompt,
// 				},
// 				{
// 					Role:    openai.ChatMessageRoleUser,
// 					Content: prompt,
// 				},
// 			},
// 			ResponseFormat: &openai.ChatCompletionResponseFormat{
// 				Type: openai.ChatCompletionResponseFormatTypeJSONSchema,
// 				JSONSchema: &openai.ChatCompletionResponseFormatJSONSchema{
// 					Name:   "something",
// 					Strict: true,
// 					Schema: jsonSchema,
// 				},
// 			},
// 		},
// 	)
// 	if err != nil {
// 		return "", err
// 	}

// 	completion := resp.Choices[0].Message.Content
// 	log.Debugf("OpenAI response: %s", completion)

// 	// Check if the response is valid in respect to the provided JSON schema
// 	validationResult := jsonSchema.ValidateJSON([]byte(completion))
// 	if validationResult.IsValid() {
// 		log.Debugf("Response is valid according to the declared schema")
// 	} else {
// 		errMsg := "Response is not valid according to the schema"
// 		if sch, err := jsonSchema.MarshalJSON(); err != nil {
// 			log.Errorf("Failed to marshal schema: %v", err)
// 		} else {
// 			log.Errorf("Schema: %s", sch)
// 		}

// 		for field, err := range validationResult.Errors {
// 			log.Errorf("Validation error in field '%s': %s", field, err)
// 			errMsg += "\n" + field + ": " + err.Error()
// 		}
// 		return "", fmt.Errorf("%s: %s", errMsg, completion)
// 	}

// 	return resp.Choices[0].Message.Content, nil
// }
