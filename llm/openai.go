package llm

import (
	"context"
	"fmt"

	"github.com/Azure/azure-sdk-for-go/sdk/azidentity"
	"github.com/invopop/jsonschema"
	openai "github.com/openai/openai-go"
	"github.com/openai/openai-go/azure"
	"github.com/openai/openai-go/option"

	log "github.com/sirupsen/logrus"
)

type OpenAIProvider struct {
	client       openai.Client
	systemPrompt string
	model        Model
}

type OpenAIProviderConfig struct {
	APIKey       string
	SystemPrompt string
	Model        Model
	BaseUrl      string
	AzureConfig  *AzureConfig // Optional, for Azure OpenAI
}

func NewOpenAIProvider(conf OpenAIProviderConfig) *OpenAIProvider {
	if conf.APIKey == "" && conf.AzureConfig == nil {
		log.Fatal("API key or Azure configuration is required for OpenAI provider")
	}

	if conf.AzureConfig != nil {
		// Create Azure OpenAI provider
		op, err := NewAzureOpenAIProvider(*conf.AzureConfig, conf.SystemPrompt, conf.Model)
		if err != nil {
			log.Fatalf("Failed to create Azure OpenAI provider: %v", err)
		}
		return op
	}
	// Create Native OpenAI provider
	if conf.APIKey == "" {
		log.Fatal("API key is required for OpenAI provider")
	}

	return NewNativeOpenAIProvider(conf)
}

func NewNativeOpenAIProvider(c OpenAIProviderConfig) *OpenAIProvider {
	opts := []option.RequestOption{}
	if c.APIKey != "" {
		log.Debugf("Setting OpenAI API key")
		opts = append(opts, option.WithAPIKey(c.APIKey))
	}
	if c.BaseUrl != "" {
		log.Debugf("Setting OpenAI base URL to: %s", c.BaseUrl)
		opts = append(opts, option.WithBaseURL(c.BaseUrl))
	}
	log.Debugf("Options for OpenAI client: %v", opts)

	client := openai.NewClient(opts...)
	return &OpenAIProvider{
		client:       client,
		systemPrompt: c.SystemPrompt,
		model:        c.Model,
	}
}

type AzureConfig struct {
	Endpoint       string
	APIVersion     string
	APIKey         string // optional, fallback to token
	DeploymentName string // azure deployment name
}

func NewAzureOpenAIProvider(cfg AzureConfig, systemPrompt string, model Model) (*OpenAIProvider, error) {
	opts := []option.RequestOption{
		azure.WithEndpoint(cfg.Endpoint, cfg.APIVersion),
	}
	if cfg.APIKey != "" {
		opts = append(opts, azure.WithAPIKey(cfg.APIKey))
	} else {
		cred, err := azidentity.NewDefaultAzureCredential(nil)
		if err != nil {
			return nil, fmt.Errorf("failed to get Azure credential: %w", err)
		}
		opts = append(opts, azure.WithTokenCredential(cred))
	}
	client := openai.NewClient(opts...)
	return &OpenAIProvider{client: client, systemPrompt: systemPrompt, model: Model{Name: cfg.DeploymentName}}, nil
}

func GenerateSchema[T any]() interface{} {
	reflector := jsonschema.Reflector{
		AllowAdditionalProperties: false,
		DoNotReference:            true,
	}
	var v T
	return reflector.Reflect(v)
}

func (p *OpenAIProvider) RequestCompletion(prompt string) (string, error) {
	log.Debugf("Requesting completion from OpenAI with prompt: %s", prompt)
	ctx := context.Background()
	messages := []openai.ChatCompletionMessageParamUnion{}

	if p.systemPrompt != "" {
		messages = append(messages, openai.SystemMessage(p.systemPrompt))
	}
	messages = append(messages, openai.UserMessage(prompt))

	params := openai.ChatCompletionNewParams{
		Messages: messages,
		Model:    p.model.Name,
	}

	resp, err := p.client.Chat.Completions.New(ctx, params)
	if err != nil {
		return "", err
	}
	return resp.Choices[0].Message.Content, nil
}

func (p *OpenAIProvider) RequestCompletionWithJSONSchema(prompt string, schema any) (string, error) {
	ctx := context.Background()
	messages := []openai.ChatCompletionMessageParamUnion{}

	if p.systemPrompt != "" {
		messages = append(messages, openai.SystemMessage(p.systemPrompt))
	}
	messages = append(messages, openai.UserMessage(prompt))

	schemaParam := openai.ResponseFormatJSONSchemaJSONSchemaParam{
		Name:   "structured_output",
		Schema: schema, // expects a struct generated by github.com/invopop/jsonschema
		Strict: openai.Bool(true),
	}

	params := openai.ChatCompletionNewParams{
		Messages: messages,
		Model:    p.model.Name,
		ResponseFormat: openai.ChatCompletionNewParamsResponseFormatUnion{
			OfJSONSchema: &openai.ResponseFormatJSONSchemaParam{
				JSONSchema: schemaParam,
			},
		},
	}

	resp, err := p.client.Chat.Completions.New(ctx, params)
	if err != nil {
		return "", err
	}
	log.Debugf("OpenAI response: %s", resp.Choices[0].Message.Content)

	// Return raw JSON, user can unmarshal as needed
	return resp.Choices[0].Message.Content, nil
}
